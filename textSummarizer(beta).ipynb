{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "textSummarizer(beta).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unknown1924/Text-Summarizer/blob/master/textSummarizer(beta).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQSYs3TXtjG8",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Er_9KyvCcWON",
        "colab_type": "code",
        "outputId": "097e7685-ee2b-4391-d136-1337983a030c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "\n",
        "fetched_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/Death_Note')\n",
        "article_read = fetched_data.read()\n",
        "\n",
        "#parsing the URL content and storing in a variable\n",
        "article_parsed = BeautifulSoup.BeautifulSoup(article_read,'html.parser')\n",
        "\n",
        "#returning <p> tags\n",
        "paragraphs = article_parsed.find_all('p')\n",
        "\n",
        "article_content = ''\n",
        "\n",
        "#looping through the paragraphs and adding them to the variable\n",
        "for p in paragraphs:  \n",
        "    article_content += p.text\n",
        "para1 = article_content\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d75385e8abdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfetched_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://en.wikipedia.org/wiki/Death_Note'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0marticle_read\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetched_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#parsing the URL content and storing in a variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'urllib' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tQASzz1Yg8Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#AN EXAMPLE TEXT\n",
        "#THIS IS HOW AN IMPORTED .TXT DOCUMENT MIGHT LOOK LIKE\n",
        "#I HAVE JUST TAKEN AN EXAMPLE OF A PARAGRAPH\n",
        "#AN ACTUALL DOCUMENT MIGHT   CONSISTS OF MULTIPLE PARAGRAPHS BUT EVENTUALLY THEY'LL BE BROKEN DOWN INTO SENTENCES(SO IT'S DOESN'T MATTERS)\n",
        "\n",
        "para = '''Google is debuting a new voice recorder app on the Pixel 4 and it has some features that very much set it apart from traditional recorder apps. The app simply called Recorder also has the ability to transcribe your recordings. That alone is uncommon among voice recorder apps let alone free ones and Google pushes that even further creating those transcriptions on device and in real time without sending data to the internet.\n",
        "\n",
        "For whatever reason Google has not made a voice recorder app before now. It has been an odd absence on Pixel phones. And while it could be remedied by downloading any number of apps from the Play Store having a simple one built into a phone should very much be an expected feature.\n",
        "The Recorder app really does seem to push beyond what most voice recorders can do though. Apple Voice Memos app for instance just records audio and stores it for playback. And while there are some excellent transcription apps out there like Otter they usually perform that transcription on a server which requires uploading your recordings to a third party then waiting for the transcription to process.\n",
        "Google is able to do it all on device thanks to a new model for processing language which Google has shrunk down so that it can be stored and run entirely on a phone. The app will even be able to identify certain sounds like applause or playing music. For now the app is only available in English but Google says more languages will come later. It will also be limited to the Pixel 4.\n",
        "The app looks impressive so far. Google showed a live demo onstage where the app transcribed what the speaker was saying and seemed to pretty much nail the transcript.\n",
        "'''\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eURSSAWbZ8cM",
        "colab_type": "code",
        "outputId": "b02c05f2-917f-4892-9da2-2a7a35f38189",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "#DOWNLOADING word2vec binary and decompressing it\n",
        "!wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
        "!gunzip -k /content/GoogleNews-vectors-negative300.bin.gz\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-18 07:42:30--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.238.165\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.238.165|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘GoogleNews-vectors-negative300.bin.gz’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  16.2MB/s    in 1m 41s  \n",
            "\n",
            "2019-11-18 07:44:12 (15.5 MB/s) - ‘GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQd89S6CZ8VS",
        "colab_type": "code",
        "outputId": "d88155ea-71af-469f-9b17-9389597187e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords') #STOPWORDS\n",
        "nltk.download('punkt')     #PUNCTUATIONS\n",
        "\n",
        "#convert paragraph to sentences\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import string\n",
        "\n",
        "#THE GOTO LIBRARY TO USE WORD2VEC\n",
        "import gensim\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YfnCmuXZ8Lr",
        "colab_type": "code",
        "outputId": "80ab5bfa-0fcf-4d3e-8e1a-c68fa45f9d8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "#NOW THE MODEL CAN BE USED BY model['your_given_word']\n",
        "\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format('/content/GoogleNews-vectors-negative300.bin', binary=True)\n",
        "\n",
        "print('Now the model is ready to be used!')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Now the model is ready to be used!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GL1AMDUzsFo2",
        "colab_type": "code",
        "outputId": "b50606e1-0203-44a5-97a3-753dcf084f9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "#CONVERTING SENTENCES TO LIST OF WORDS!!!!(IMPORTANT)\n",
        "\n",
        "sentences = sent_tokenize(para)\n",
        "no_of_sentences = len(sentences)\n",
        "\n",
        "words = []\n",
        "for i in range(no_of_sentences):\n",
        "  #tokenize each sentence to individual words and add it to words[i]\n",
        "  \n",
        "  words.append(word_tokenize(sentences[i])) \n",
        "  \n",
        "  #remove the  last element that is '.'\n",
        "  \n",
        "  remove_full_stop(words[i]) \n",
        "  \n",
        "#removing full stops at the end of each sentences  \n",
        "for i in range(len(sentences)):\n",
        "  sentences[i] = sentences[i].strip('.')\n",
        "  \n",
        "for i in range(no_of_sentences):\n",
        "  print(words[i])\n",
        "  print(\"-------------------------------------------------------------------\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-f25bcc12a2d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;31m#remove the  last element that is '.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0mremove_full_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#removing full stops at the end of each sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'remove_full_stop' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sonZ7jgu1vZU",
        "colab_type": "code",
        "outputId": "ed283c12-d200-4834-b6dd-0e40c11f0de6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        }
      },
      "source": [
        "#JUST TEMPORARY\n",
        "#IS OF NO USE\n",
        "\n",
        "index = 1\n",
        "\n",
        "for i in range(len(words[index])):\n",
        "  print(i,' -- ',words[index][i])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-6fe9203c86a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' -- '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knbA58WGwagT",
        "colab_type": "code",
        "outputId": "6da7ae39-6342-4b00-dada-75c4c359e1b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "#REMOVE STOPWORDS LIKE 'a', 'the', blah blah...\n",
        "\n",
        "for i in range(no_of_sentences):\n",
        "  words[i] = [word for word in words[i] if word not in stopwords.words('english')]  \n",
        "  #words[i] = [word for word in words[i] if word not in punkt.words('english')] \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-9818e6743df8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_of_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0;31m#words[i] = [word for word in words[i] if word not in punkt.words('english')]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwXgR86maDcr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#NOW CREATE A NEW NP ARRAY TO STORE THE WORD VECTORS CORRESPONDING TO EACH WORDS IN words[i]\n",
        "\n",
        "word_vec = []\n",
        "for i in range(no_of_sentences):\n",
        "  \n",
        "  #sent_word_vec will store the word vectors of each word in each sentence\n",
        "  \n",
        "  sent_word_vec = []\n",
        "  for j in range(len(words[i])):\n",
        "    np_vec = np.asarray(model[words[i][j]])\n",
        "    sent_word_vec.append(np_vec)\n",
        "  \n",
        "  #however word_vec will store the list of sent_word_vec so basically\n",
        "  #word_vec[0] will represent the sentence word vectors and \n",
        "  #word_vec[0][0] will represent the word vector of first word in first sentence\n",
        "  \n",
        "  word_vec.append(sent_word_vec)  \n",
        "    \n",
        "#    word_vec[i][j]  / i = index of sentence\n",
        "#                    \\ j = index of word within i th sentence\n",
        "len(word_vec[0][0])\n",
        "\n",
        "#word_vec\n",
        "#all_word_vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zm3qWCBzFGpy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#COLLECTION OF ALL THE WORDS IN THE ENTIRE PARAGRAPH\n",
        "#all_words contains collection of all the words in the paragraph\n",
        "\n",
        "all_words = []\n",
        "\n",
        "for i in range(no_of_sentences):\n",
        "  for j in range(len(words[i])):\n",
        "    all_words.append(words[i][j])\n",
        "\n",
        "    \n",
        "#all_word_vec contains the word vector representation of all the words\n",
        "#in the paragraph\n",
        "\n",
        "all_word_vec = []\n",
        "for i in range(len(all_words)):\n",
        "  all_word_vec.append(model[all_words[i]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLmjSPTUGhpM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_word_vec = np.asarray(all_word_vec)\n",
        "print(np.shape(all_word_vec),'\\n',type(all_word_vec))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_OauFU8-XdE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYrp8mMkaDSa",
        "colab_type": "code",
        "outputId": "94566f47-dc32-45ce-fea4-3803272dc227",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "#WILL SHOW THE TOP 10 MOST SIMILAR WORDS CORRESPONDING TO THE GIVEN ONE\\\n",
        "\n",
        "print(model.most_similar([word_vec[1][8]]))\n",
        "print(\"n\\n\\nword_vec[1][8] corresponds to : '\",words[1][8],\"'\\n\")\n",
        "print(\"\\n\\nThis is what word_vec[1][8] actually looks like:\\n \")\n",
        "print(word_vec[1][8])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-ea404dae1064>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"n\\n\\nword_vec[1][8] corresponds to : '\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"'\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\nThis is what word_vec[1][8] actually looks like:\\n \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'word_vec' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdB6j7-NZ3As",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#JUST CHECKING SIMILARITIES\n",
        "\n",
        "print(\"Similarity:\\n\")\n",
        "model.similarity(words[1][7],words[1][8])\n",
        "#                    👆           👆\n",
        "# corresponds to \"transcribe\" & \"recordings\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RODZY5QPMGZ0",
        "colab_type": "text"
      },
      "source": [
        "### **TESTING**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqeFkA49-vu-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kmeans = KMeans(n_clusters=20)\n",
        "kmeans.fit(all_word_vec)\n",
        "labels = kmeans.labels_\n",
        "centroids = kmeans.cluster_centers_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waBnvA8lHL1v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ca,cb = 18,20\n",
        "print(all_words[ca],' ',all_words[cb],' ',all_words[10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alh5tAn2Iaw3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(np.shape(centroids))\n",
        "print(np.shape(labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVIutAutIVOG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.scatter(centroids[:,0],centroids[:,1], s = 30, color = 'g')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOKBhry2ID7l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.unique()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VN-kRngUAvWo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(170):\n",
        "  print(i,\"--\",labels[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6c-m5TcwC0py",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "temp_vec = np.asarray(word_vec)\n",
        "\n",
        "\n",
        "X, y = make_blobs(n_samples = 300, centers = 15,  \n",
        "                cluster_std = 1, n_features = 2) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yfX5bKaEOF7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.shape(word_vec[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_5Ro6hpC2Za",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.shape(temp_vec[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzqtUA2lEzWk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t = temp_vec[0]\n",
        "np.shape(t)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZem0ees-Gkc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#plt.scatter(X[:, 0], X[:, 1], s = 30, color ='b')\n",
        "\n",
        "plt.scatter(all_word_vec[:,0],all_word_vec[:,1], s = 30, color = 'g')\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRZgPeeLIjsz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#OPTIMAL NUMBER OF CLUSTERS COMES OUT TO BE 124 OR 125 [WHATEVER]\n",
        "\n",
        "no_of_iteration = 124\n",
        "cost =[] \n",
        "for i in range(1, no_of_iteration+1): \n",
        "    KM = KMeans(n_clusters = i, max_iter = 500) \n",
        "    KM.fit(all_word_vec) \n",
        "      \n",
        "    # calculates squared error \n",
        "    # for the clustered points \n",
        "    cost.append(KM.inertia_)      \n",
        "  \n",
        "# plot the cost against K values \n",
        "plt.plot(range(1, no_of_iteration+1), cost, color ='g', linewidth ='3') \n",
        "plt.xlabel(\"Value of K\") \n",
        "plt.ylabel(\"Sqaured Error (Cost)\") \n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmxZw8XzPieL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#NOW FIND SIMILARITY OF THE SENTENCES WITH THE FORMED CLUSTERS BASED ON no of common words/|sentence| sorta..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96Uzg01BPfIk",
        "colab_type": "text"
      },
      "source": [
        "### **TESTING ENDS HERE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nnqbVpu7SVE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#BEFORE CLUSTERING WORDS WE HAVE TO FIND THE OPTIMAL NUMBER OF CLUSTERS\n",
        "#THIS K VALUE CAN BE FOUND BY USING \"ELBOW METHOD\"\n",
        "#WE WILL USE matplotlib TO VISUALLY ASSERT THE VALUE OF K IN K-Means algorithm\n",
        "\n",
        "#Elbow Method\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JosQ4J_i6sR9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#CLUSTER THE GIVEN WORD VECTORS INTO CLUSTERS USING k-Means algorigthm\n",
        "#sklearn library already has a built in function to implement k-Means algorithm\n",
        "\n",
        "K = KMeans()\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}