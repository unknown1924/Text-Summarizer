# -*- coding: utf-8 -*-
"""textSummarizer(beta).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Rugbhey716krh1GdOCkUuLMgyCi8axLw
"""

fetched_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/Death_Note')
article_read = fetched_data.read()

#parsing the URL content and storing in a variable
article_parsed = BeautifulSoup.BeautifulSoup(article_read,'html.parser')

#returning <p> tags
paragraphs = article_parsed.find_all('p')

article_content = ''

#looping through the paragraphs and adding them to the variable
for p in paragraphs:  
    article_content += p.text
para1 = article_content

#AN EXAMPLE TEXT
#THIS IS HOW AN IMPORTED .TXT DOCUMENT MIGHT LOOK LIKE
#I HAVE JUST TAKEN AN EXAMPLE OF A PARAGRAPH
#AN ACTUALL DOCUMENT MIGHT   CONSISTS OF MULTIPLE PARAGRAPHS BUT EVENTUALLY THEY'LL BE BROKEN DOWN INTO SENTENCES(SO IT'S DOESN'T MATTERS)

para = '''Google is debuting a new voice recorder app on the Pixel 4 and it has some features that very much set it apart from traditional recorder apps. The app simply called Recorder also has the ability to transcribe your recordings. That alone is uncommon among voice recorder apps let alone free ones and Google pushes that even further creating those transcriptions on device and in real time without sending data to the internet.

For whatever reason Google has not made a voice recorder app before now. It has been an odd absence on Pixel phones. And while it could be remedied by downloading any number of apps from the Play Store having a simple one built into a phone should very much be an expected feature.
The Recorder app really does seem to push beyond what most voice recorders can do though. Apple Voice Memos app for instance just records audio and stores it for playback. And while there are some excellent transcription apps out there like Otter they usually perform that transcription on a server which requires uploading your recordings to a third party then waiting for the transcription to process.
Google is able to do it all on device thanks to a new model for processing language which Google has shrunk down so that it can be stored and run entirely on a phone. The app will even be able to identify certain sounds like applause or playing music. For now the app is only available in English but Google says more languages will come later. It will also be limited to the Pixel 4.
The app looks impressive so far. Google showed a live demo onstage where the app transcribed what the speaker was saying and seemed to pretty much nail the transcript.
'''

#DOWNLOADING word2vec binary and decompressing it
!wget -c "https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz"
!gunzip -k /content/GoogleNews-vectors-negative300.bin.gz

import matplotlib.pyplot as plt
from matplotlib import style

from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans

import numpy as np
import nltk
from nltk.corpus import stopwords

nltk.download('stopwords') #STOPWORDS
nltk.download('punkt')     #PUNCTUATIONS

#convert paragraph to sentences
from nltk.tokenize import sent_tokenize, word_tokenize
import string

#THE GOTO LIBRARY TO USE WORD2VEC
import gensim

#NOW THE MODEL CAN BE USED BY model['your_given_word']

model = gensim.models.KeyedVectors.load_word2vec_format('/content/GoogleNews-vectors-negative300.bin', binary=True)

print('Now the model is ready to be used!')

#CONVERTING SENTENCES TO LIST OF WORDS!!!!(IMPORTANT)

sentences = sent_tokenize(para)
no_of_sentences = len(sentences)

words = []
for i in range(no_of_sentences):
  #tokenize each sentence to individual words and add it to words[i]
  
  words.append(word_tokenize(sentences[i])) 
  
  #remove the  last element that is '.'
  
  remove_full_stop(words[i]) 
  
#removing full stops at the end of each sentences  
for i in range(len(sentences)):
  sentences[i] = sentences[i].strip('.')
  
for i in range(no_of_sentences):
  print(words[i])
  print("-------------------------------------------------------------------")

#JUST TEMPORARY
#IS OF NO USE

index = 1

for i in range(len(words[index])):
  print(i,' -- ',words[index][i])

#REMOVE STOPWORDS LIKE 'a', 'the', blah blah...

for i in range(no_of_sentences):
  words[i] = [word for word in words[i] if word not in stopwords.words('english')]  
  #words[i] = [word for word in words[i] if word not in punkt.words('english')]

#NOW CREATE A NEW NP ARRAY TO STORE THE WORD VECTORS CORRESPONDING TO EACH WORDS IN words[i]

word_vec = []
for i in range(no_of_sentences):
  
  #sent_word_vec will store the word vectors of each word in each sentence
  
  sent_word_vec = []
  for j in range(len(words[i])):
    np_vec = np.asarray(model[words[i][j]])
    sent_word_vec.append(np_vec)
  
  #however word_vec will store the list of sent_word_vec so basically
  #word_vec[0] will represent the sentence word vectors and 
  #word_vec[0][0] will represent the word vector of first word in first sentence
  
  word_vec.append(sent_word_vec)  
    
#    word_vec[i][j]  / i = index of sentence
#                    \ j = index of word within i th sentence
len(word_vec[0][0])

#word_vec
#all_word_vec

#COLLECTION OF ALL THE WORDS IN THE ENTIRE PARAGRAPH
#all_words contains collection of all the words in the paragraph

all_words = []

for i in range(no_of_sentences):
  for j in range(len(words[i])):
    all_words.append(words[i][j])

    
#all_word_vec contains the word vector representation of all the words
#in the paragraph

all_word_vec = []
for i in range(len(all_words)):
  all_word_vec.append(model[all_words[i]])

all_word_vec = np.asarray(all_word_vec)
print(np.shape(all_word_vec),'\n',type(all_word_vec))



#WILL SHOW THE TOP 10 MOST SIMILAR WORDS CORRESPONDING TO THE GIVEN ONE\

print(model.most_similar([word_vec[1][8]]))
print("n\n\nword_vec[1][8] corresponds to : '",words[1][8],"'\n")
print("\n\nThis is what word_vec[1][8] actually looks like:\n ")
print(word_vec[1][8])

#JUST CHECKING SIMILARITIES

print("Similarity:\n")
model.similarity(words[1][7],words[1][8])
#                    ðŸ‘†           ðŸ‘†
# corresponds to "transcribe" & "recordings"

"""### **TESTING**"""

kmeans = KMeans(n_clusters=20)
kmeans.fit(all_word_vec)
labels = kmeans.labels_
centroids = kmeans.cluster_centers_

ca,cb = 18,20
print(all_words[ca],' ',all_words[cb],' ',all_words[10])

print(np.shape(centroids))
print(np.shape(labels))

plt.scatter(centroids[:,0],centroids[:,1], s = 30, color = 'g')
plt.show()

np.unique()

for i in range(170):
  print(i,"--",labels[i])

temp_vec = np.asarray(word_vec)


X, y = make_blobs(n_samples = 300, centers = 15,  
                cluster_std = 1, n_features = 2)

np.shape(word_vec[0])

np.shape(temp_vec[0])

t = temp_vec[0]
np.shape(t)

#plt.scatter(X[:, 0], X[:, 1], s = 30, color ='b')

plt.scatter(all_word_vec[:,0],all_word_vec[:,1], s = 30, color = 'g')
plt.show()

#OPTIMAL NUMBER OF CLUSTERS COMES OUT TO BE 124 OR 125 [WHATEVER]

no_of_iteration = 124
cost =[] 
for i in range(1, no_of_iteration+1): 
    KM = KMeans(n_clusters = i, max_iter = 500) 
    KM.fit(all_word_vec) 
      
    # calculates squared error 
    # for the clustered points 
    cost.append(KM.inertia_)      
  
# plot the cost against K values 
plt.plot(range(1, no_of_iteration+1), cost, color ='g', linewidth ='3') 
plt.xlabel("Value of K") 
plt.ylabel("Sqaured Error (Cost)") 
plt.show()

#NOW FIND SIMILARITY OF THE SENTENCES WITH THE FORMED CLUSTERS BASED ON no of common words/|sentence| sorta...

"""### **TESTING ENDS HERE**"""

#BEFORE CLUSTERING WORDS WE HAVE TO FIND THE OPTIMAL NUMBER OF CLUSTERS
#THIS K VALUE CAN BE FOUND BY USING "ELBOW METHOD"
#WE WILL USE matplotlib TO VISUALLY ASSERT THE VALUE OF K IN K-Means algorithm

#Elbow Method

#CLUSTER THE GIVEN WORD VECTORS INTO CLUSTERS USING k-Means algorigthm
#sklearn library already has a built in function to implement k-Means algorithm

K = KMeans()